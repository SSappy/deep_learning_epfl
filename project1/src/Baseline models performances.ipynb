{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-414144362cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: /home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_models import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'item'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-139fa7af76d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Courses/Deep learning/Projects/project1/src/nnmodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, targets, epochs, batch_size, optimizer, lr, momentum)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1999\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# print every 2000 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     print('[%d, %5d] loss: %.3f' %\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'item'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models performances\n",
    "\n",
    "In this notebook are trained the baseline models and we use cross-validation to get the best hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/91290/how-do-i-train-hmms-for-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5b8c1df5d8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloading_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaseline_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Courses/Deep learning/Projects/project1/src/loading_helper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlc_bci\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbci\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Courses/Deep learning/Projects/project1/src/dlc_bci.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is distributed under BSD 3-Clause license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: /home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from loading_helper import load_data\n",
    "from baseline_model import BaselineModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-414144362cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: /home/quentin/miniconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: cuGetErrorString"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_khz = False\n",
    "\n",
    "x_train, y_train = load_data(one_khz=one_khz)\n",
    "x_test, y_test = load_data(train=False, one_khz=one_khz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc692688828>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BaselineModel(model='logistic')\n",
    "\n",
    "params = dict(C=np.logspace(-5, 5, 15))\n",
    "\n",
    "clf = model.tune_params(params, x_train, y_train)\n",
    "\n",
    "plt.semilogx(params['C'], clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(model='logistic')\n",
    "\n",
    "params = dict(C=np.logspace(-5, 5, 15))\n",
    "\n",
    "clf = model.tune_params(params, x_train, y_train)\n",
    "\n",
    "#plt.semilogx(params['C'], clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.pipeline.Pipeline.score>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(model='svm')\n",
    "\n",
    "params = dict(C=np.logspace(-5, 5, 15))\n",
    "\n",
    "clf = model.tune_params(params, x_train, y_train)\n",
    "\n",
    "plt.semilogx(params['C'], clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(model='forest')\n",
    "\n",
    "params = dict(C=np.logspace(-5, 5, 15))\n",
    "\n",
    "clf = model.tune_params(params, x_train, y_train)\n",
    "\n",
    "plt.semilogx(params['C'], clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(model='svm', kernel='linear', C=1)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 74.0%\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(x_test)\n",
    "\n",
    "print('Accuracy : {}%'.format(100*sum(y_hat == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6805779569892472, 0.02674447292179724)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaselineModel(model='svm', kernel='linear').cross_validation(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 49.0%\n"
     ]
    }
   ],
   "source": [
    "model = BaselineModel(model='svm', kernel='linear', C=1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_hat = model.predict(x_test)\n",
    "\n",
    "print('Accuracy : {}%'.format(100*sum(y_hat == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6774529569892472, 0.030106129991208465)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaselineModel(model='svm', kernel='linear').cross_validation(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = x_train.view(x_train.shape[0], -1)\n",
    "data = data.numpy().astype(float)\n",
    "\n",
    "targets = y_train.numpy().astype(float)\n",
    "\n",
    "scalar = preprocessing.StandardScaler()\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "pipeline = Pipeline([('transformer', scalar), ('estimator', clf)])\n",
    "\n",
    "cv = KFold(n_splits=4)\n",
    "scores = cross_val_score(pipeline, data, targets, cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67088608, 0.69620253, 0.6835443 , 0.67088608])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "from hmmlearn.hmm import GMMHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "data = x_train.view(x_train.shape[0], -1)\n",
    "data = data.numpy().astype(float)\n",
    "\n",
    "targets = y_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 49.36708860759494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quentin/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function distribute_covar_matrix_to_match_covariance_type is deprecated; The function distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = GaussianHMM(n_components=2, n_iter=10000).fit(data)\n",
    "\n",
    "# Predict the optimal sequence of internal hidden state\n",
    "hidden_states = model.predict(data)\n",
    "\n",
    "print('Accuracy : {}%'.format(100*sum(hidden_states == targets)/len(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix\n",
      "[[0.17283951 0.33333333 0.39506173 0.09876543]\n",
      " [0.31111111 0.26666667 0.3        0.12222222]\n",
      " [0.26956522 0.28695652 0.4        0.04347826]\n",
      " [0.27586207 0.20689655 0.34482759 0.17241379]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Transition matrix\")\n",
    "print(model.transmat_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means and vars of each hidden state\n",
      "0th hidden state\n",
      "mean =  [ 70.79259264  67.9592593   67.23827165 ... -11.51975316 -11.58765424\n",
      " -10.71851848]\n",
      "var =  [676.95019117 783.98648428 637.20914924 ... 368.14491927 387.9335433\n",
      " 438.04657025]\n",
      "1th hidden state\n",
      "mean =  [78.21868144 79.90439585 78.36593439 ... 39.00659333 39.96593421\n",
      " 40.77032968]\n",
      "var =  [450.54382827 536.30998452 547.24786278 ... 542.57106365 420.11532134\n",
      " 515.49912092]\n",
      "2th hidden state\n",
      "mean =  [73.5165215  74.17391316 73.16695635 ... 14.76521739 14.15391314\n",
      " 12.53739129]\n",
      "var =  [585.12562052 540.66096498 603.60872587 ... 268.94479258 329.90205565\n",
      " 368.99547086]\n",
      "3th hidden state\n",
      "mean =  [ 61.84137897  61.94827554  60.71034415 ... -51.71034475 -50.67586164\n",
      " -51.60344824]\n",
      "var =  [1719.59449654 1972.57524108 2068.30190365 ...  848.705371    904.88490597\n",
      " 1002.65791901]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Means and vars of each hidden state\")\n",
    "for i in range(model.n_components):\n",
    "    print(\"{0}th hidden state\".format(i))\n",
    "    print(\"mean = \", model.means_[i])\n",
    "    print(\"var = \", np.diag(model.covars_[i]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIG TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "index0 = np.where(targets == 0)[0]\n",
    "index1 = np.where(targets == 1)[0]\n",
    "\n",
    "data0 = data[index0]\n",
    "data1 = data[index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 1400)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = GMMHMM(n_components=3).fit(data0)\n",
    "model1 = GMMHMM(n_components=3).fit(data1)\n",
    "\n",
    "# Predict the optimal sequence of internal hidden state\n",
    "hidden_states = model0.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for x in data1:\n",
    "    res.append(0 if model0.score([x]) > model1.score([x]) else 1)\n",
    "res = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6942675159235668"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res == 0)/len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module hmmlearn.hmm in hmmlearn:\n",
      "\n",
      "NAME\n",
      "    hmmlearn.hmm - The :mod:`hmmlearn.hmm` module implements hidden Markov models.\n",
      "\n",
      "CLASSES\n",
      "    hmmlearn.base._BaseHMM(sklearn.base.BaseEstimator)\n",
      "        GMMHMM\n",
      "        GaussianHMM\n",
      "        MultinomialHMM\n",
      "    \n",
      "    class GMMHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with Gaussian mixture emissions.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int\n",
      "     |      Number of states in the model.\n",
      "     |  \n",
      "     |  n_mix : int\n",
      "     |      Number of states in the GMM.\n",
      "     |  \n",
      "     |  covariance_type : string, optional\n",
      "     |      String describing the type of covariance parameters to\n",
      "     |      use.  Must be one of\n",
      "     |  \n",
      "     |      * \"spherical\" --- each state uses a single variance value that\n",
      "     |        applies to all features.\n",
      "     |      * \"diag\" --- each state uses a diagonal covariance matrix.\n",
      "     |      * \"full\" --- each state uses a full (i.e. unrestricted)\n",
      "     |        covariance matrix.\n",
      "     |      * \"tied\" --- all states use **the same** full covariance matrix.\n",
      "     |  \n",
      "     |      Defaults to \"diag\".\n",
      "     |  \n",
      "     |  min_covar : float, optional\n",
      "     |      Floor on the diagonal of the covariance matrix to prevent\n",
      "     |      overfitting. Defaults to 1e-3.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  weights_prior : array, shape (n_mix, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`weights_`.\n",
      "     |  \n",
      "     |  means_prior, means_weight : array, shape (n_mix, ), optional\n",
      "     |      Mean and precision of the Normal prior distribtion for\n",
      "     |      :attr:`means_`.\n",
      "     |  \n",
      "     |  covars_prior, covars_weight : array, shape (n_mix, ), optional\n",
      "     |      Parameters of the prior distribution for the covariance matrix\n",
      "     |      :attr:`covars_`.\n",
      "     |  \n",
      "     |      If :attr:`covariance_type` is \"spherical\" or \"diag\" the prior is\n",
      "     |      the inverse gamma distribution, otherwise --- the inverse Wishart\n",
      "     |      distribution.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to training. Can\n",
      "     |      contain any combination of 's' for startprob, 't' for transmat, 'm'\n",
      "     |      for means, 'c' for covars, and 'w' for GMM mixing weights.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training process.  Can\n",
      "     |      contain any combination of 's' for startprob, 't' for transmat, 'm' for\n",
      "     |      means, and 'c' for covars, and 'w' for GMM mixing weights.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  weights\\_ : array, shape (n_components, n_mix)\n",
      "     |      Mixture weights for each state.\n",
      "     |  \n",
      "     |  means\\_ : array, shape (n_components, n_mix)\n",
      "     |      Mean parameters for each mixture component in each state.\n",
      "     |  \n",
      "     |  covars\\_ : array\n",
      "     |      Covariance parameters for each mixture components in each state.\n",
      "     |  \n",
      "     |      The shape depends on :attr:`covariance_type`::\n",
      "     |  \n",
      "     |          (n_components, n_mix)                          if \"spherical\",\n",
      "     |          (n_components, n_features, n_features)         if \"tied\",\n",
      "     |          (n_components, n_mix, n_features)              if \"diag\",\n",
      "     |          (n_components, n_mix, n_features, n_features)  if \"full\"\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GMMHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, n_mix=1, min_covar=0.001, startprob_prior=1.0, transmat_prior=1.0, weights_prior=1.0, means_prior=0.0, means_weight=0.0, covars_prior=None, covars_weight=None, algorithm='viterbi', covariance_type='diag', random_state=None, n_iter=10, tol=0.01, verbose=False, params='stmcw', init_params='stmcw')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GaussianHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with Gaussian emissions.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int\n",
      "     |      Number of states.\n",
      "     |  \n",
      "     |  covariance_type : string, optional\n",
      "     |      String describing the type of covariance parameters to\n",
      "     |      use.  Must be one of\n",
      "     |  \n",
      "     |      * \"spherical\" --- each state uses a single variance value that\n",
      "     |        applies to all features.\n",
      "     |      * \"diag\" --- each state uses a diagonal covariance matrix.\n",
      "     |      * \"full\" --- each state uses a full (i.e. unrestricted)\n",
      "     |        covariance matrix.\n",
      "     |      * \"tied\" --- all states use **the same** full covariance matrix.\n",
      "     |  \n",
      "     |      Defaults to \"diag\".\n",
      "     |  \n",
      "     |  min_covar : float, optional\n",
      "     |      Floor on the diagonal of the covariance matrix to prevent\n",
      "     |      overfitting. Defaults to 1e-3.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  means_prior, means_weight : array, shape (n_components, ), optional\n",
      "     |      Mean and precision of the Normal prior distribtion for\n",
      "     |      :attr:`means_`.\n",
      "     |  \n",
      "     |  covars_prior, covars_weight : array, shape (n_components, ), optional\n",
      "     |      Parameters of the prior distribution for the covariance matrix\n",
      "     |      :attr:`covars_`.\n",
      "     |  \n",
      "     |      If :attr:`covariance_type` is \"spherical\" or \"diag\" the prior is\n",
      "     |      the inverse gamma distribution, otherwise --- the inverse Wishart\n",
      "     |      distribution.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or`\"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training\n",
      "     |      process.  Can contain any combination of 's' for startprob,\n",
      "     |      't' for transmat, 'm' for means and 'c' for covars. Defaults\n",
      "     |      to all parameters.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to\n",
      "     |      training.  Can contain any combination of 's' for\n",
      "     |      startprob, 't' for transmat, 'm' for means and 'c' for covars.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features : int\n",
      "     |      Dimensionality of the Gaussian emissions.\n",
      "     |  \n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  means\\_ : array, shape (n_components, n_features)\n",
      "     |      Mean parameters for each state.\n",
      "     |  \n",
      "     |  covars\\_ : array\n",
      "     |      Covariance parameters for each state.\n",
      "     |  \n",
      "     |      The shape depends on :attr:`covariance_type`::\n",
      "     |  \n",
      "     |          (n_components, )                        if \"spherical\",\n",
      "     |          (n_features, n_features)                if \"tied\",\n",
      "     |          (n_components, n_features)              if \"diag\",\n",
      "     |          (n_components, n_features, n_features)  if \"full\"\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from hmmlearn.hmm import GaussianHMM\n",
      "     |  >>> GaussianHMM(n_components=2)\n",
      "     |  ...                             #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
      "     |  GaussianHMM(algorithm='viterbi',...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, covariance_type='diag', min_covar=0.001, startprob_prior=1.0, transmat_prior=1.0, means_prior=0, means_weight=0, covars_prior=0.01, covars_weight=1, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='stmc', init_params='stmc')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  covars_\n",
      "     |      Return covars as a full matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultinomialHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with multinomial (discrete) emissions\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_components : int\n",
      "     |      Number of states.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training\n",
      "     |      process.  Can contain any combination of 's' for startprob,\n",
      "     |      't' for transmat, 'e' for emissionprob.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to\n",
      "     |      training.  Can contain any combination of 's' for\n",
      "     |      startprob, 't' for transmat, 'e' for emissionprob.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features : int\n",
      "     |      Number of possible symbols emitted by the model (in the samples).\n",
      "     |  \n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  emissionprob\\_ : array, shape (n_components, n_features)\n",
      "     |      Probability of emitting a given symbol when in each state.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from hmmlearn.hmm import MultinomialHMM\n",
      "     |  >>> MultinomialHMM(n_components=2)\n",
      "     |  ...                             #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
      "     |  MultinomialHMM(algorithm='viterbi',...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultinomialHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, startprob_prior=1.0, transmat_prior=1.0, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='ste', init_params='ste')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['GMMHMM', 'GaussianHMM', 'MultinomialHMM']\n",
      "\n",
      "FILE\n",
      "    /home/quentin/miniconda3/lib/python3.6/site-packages/hmmlearn-0.2.1-py3.6-linux-x86_64.egg/hmmlearn/hmm.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hmmlearn\n",
    "help(hmmlearn.hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
